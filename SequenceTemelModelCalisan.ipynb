{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SequenceTemelModelCalisan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9CZCminQyjM+AExB0gwb6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pojo-25/drugProject/blob/main/SequenceTemelModelCalisan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJbcfZ00IyEQ"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, confusion_matrix\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdjLBmERJejm"
      },
      "source": [
        "# data_path = '/content/data/HIV.csv'\n",
        "# col_smiles = 'smiles'\n",
        "# col_target = 'HIV_active'\n",
        "\n",
        "\n",
        "# # read data\n",
        "# df = pd.read_csv(data_path, sep=',')\n",
        "# df_no_na = df[[col_smiles, col_target]].dropna()\n",
        "\n",
        "# X = df_no_na[col_smiles]\n",
        "# y = df_no_na[col_target].values\n",
        "\n",
        "# print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLkTBRI_KaaL"
      },
      "source": [
        "col_smiles = 'smiles'\n",
        "col_target = 'HIV_active'\n",
        "\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "METRIC_F1_SCORE = 'f1-score'\n",
        "METRIC_COHEN_KAPPA = 'Cohen kappa'\n",
        "METRIC_CONFUSION_MATRIX = 'Confusion Matrix'\n",
        "\n",
        "\n",
        "CLASSES = ['benign', 'malignant']\n",
        "TEST_RATIO = 0.2\n",
        "SEED = 0\n",
        "\n",
        "data_path = '/content/data/HIV.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R-SLqIZJQmb"
      },
      "source": [
        "def read_data(data_path, col_smiles='smiles', col_target='HIV_active'):\n",
        "    \"\"\"Split original data into train data and test data.\n",
        "    :param data_path: str, path to the a CSV data file\n",
        "    :param col_smiles: str, name of smiles column\n",
        "    :param col_target: str, name of target column\n",
        "    :param test_ratio: float, proportion of the original data for testset, must be from 0 to 1\n",
        "    :param seed: int, randomization seed for reproducibility\n",
        "    :return (X, y)\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    # read data\n",
        "    df = pd.read_csv(data_path, sep=',')\n",
        "    df_no_na = df[[col_smiles, col_target]].dropna()\n",
        "\n",
        "    X = df_no_na[col_smiles]\n",
        "    y = df_no_na[col_target].values\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "                \n",
        "def get_prediction_score(y_label, y_predict):\n",
        "    \"\"\"Evaluate predictions using different evaluation metrics.\n",
        "    :param y_label: list, contains true label\n",
        "    :param y_predict: list, contains predicted label\n",
        "    :return scores: dict, evaluation metrics on the prediction\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "    scores[METRIC_ACCURACY] = accuracy_score(y_label, y_predict)\n",
        "    scores[METRIC_F1_SCORE] = f1_score(y_label, y_predict, labels=None, average='macro', sample_weight=None)\n",
        "    scores[METRIC_COHEN_KAPPA] = cohen_kappa_score(y_label, y_predict)\n",
        "    scores[METRIC_CONFUSION_MATRIX] = confusion_matrix(y_label, y_predict)\n",
        "    \n",
        "    return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gznEWrCBO0WL"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.insert(0, os.getcwd()) # add current working directory to pythonpath\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras import backend as K\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # Use only the 1st GPU\n",
        "tf_config = tf.compat.v1.ConfigProto()\n",
        "sess = tf.compat.v1.Session(config=tf_config)\n",
        "K.set_session(sess)\n",
        "\n",
        "\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Flatten\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GlobalMaxPooling1D, SpatialDropout1D, Conv1D, MaxPooling1D, GRU, Bidirectional\n",
        "from tensorflow.keras import optimizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "import warnings\n",
        "import gc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qixl8sQ3KNue"
      },
      "source": [
        "# import os\n",
        "# import sys\n",
        "# sys.path.insert(0, os.getcwd()) # add current working directory to pythonpath\n",
        "\n",
        "\n",
        "\n",
        "# import keras.backend as K\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from keras import callbacks\n",
        "# from keras.optimizers import Adam\n",
        "# from keras.models import load_model\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from keras.models import Sequential\n",
        "# from keras.models import Model\n",
        "# from keras.layers import Input, Dense, Dropout, BatchNormalization, Flatten\n",
        "# from keras.layers import Embedding, LSTM, GlobalMaxPooling1D, SpatialDropout1D, Conv1D, MaxPooling1D, GRU, Bidirectional\n",
        "# from keras import optimizers\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.utils import class_weight\n",
        "\n",
        "# import warnings\n",
        "# import argparse\n",
        "# import gc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCFRYRiQNe5S"
      },
      "source": [
        "def generate_tokens(smiles, len_percentile=100):\n",
        "    \"\"\"\n",
        "    Generate character tokens from smiles\n",
        "    :param smiles: Pandas series, containing smiles\n",
        "    :param len_percentile: percentile of smiles length to set as max length\n",
        "    :return tokens\n",
        "    :return num_words\n",
        "    :return max_phrase_len\n",
        "    \"\"\" \n",
        "    \n",
        "    # Get max length of smiles\n",
        "    smiles_len = smiles.apply(lambda p: len(p))\n",
        "    max_phrase_len = int(np.percentile(smiles_len, len_percentile))\n",
        "    print('True max length is ' + str(np.max(smiles_len)) + ', ' + str(max_phrase_len) + ' is set the length cutoff.')\n",
        "        \n",
        "    # Get unique words\n",
        "    unique_words = np.unique(np.concatenate(smiles.apply(lambda p: np.array(list(p))).values, axis=0))\n",
        "    num_words = len(unique_words)\n",
        "    print('Vocab size is ' + str(num_words))\n",
        "    \n",
        "    tokenizer = Tokenizer(\n",
        "        num_words = num_words,\n",
        "        filters = '$',\n",
        "        char_level = True,\n",
        "        oov_token = '_'\n",
        "    )\n",
        "\n",
        "    #print(num_words)\n",
        "    \n",
        "    tokenizer.fit_on_texts(smiles)\n",
        "    sequences = tokenizer.texts_to_sequences(smiles)\n",
        "    tokens = pad_sequences(sequences, maxlen = max_phrase_len, padding='post', truncating='post')\n",
        "    \n",
        "    return tokens, num_words, max_phrase_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0ELiElPNi1y",
        "outputId": "3cceace3-28d5-465e-f8c9-ec741fa43c87"
      },
      "source": [
        "smiles, y = read_data(data_path, col_smiles='smiles', col_target='HIV_active')\n",
        "tokens, num_words, max_phrase_len = generate_tokens(smiles, len_percentile=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True max length is 580, 580 is set the length cutoff.\n",
            "Vocab size is 56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUZXhoIKQEe0"
      },
      "source": [
        "def create_model(model_type, num_words, input_length, output_dim=1, dropout_rate=0.0):\n",
        "    \"\"\"Build different sequence model\n",
        "    :param model_type: str, can be 'cnn-gru', 'cnn', 'gru', 'lstm'\n",
        "    :param num_words: int\n",
        "    :param input_length: int\n",
        "    :param output_dim: int\n",
        "    :return model: Keras model\n",
        "    \"\"\" \n",
        "    \n",
        "    model = Sequential()\n",
        "    if model_type == 'lstm': # LSTM - LSTM\n",
        "        model.add(Embedding(num_words+1, 50, input_length=input_length))\n",
        "        model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "        model.add(Bidirectional(LSTM(128)))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "        model.add(Dense(output_dim, activation='sigmoid'))\n",
        "    elif model_type == 'gru': # GRU - GRU\n",
        "        model.add(Embedding(num_words+1, 50, input_length=input_length))\n",
        "        model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
        "        model.add(Bidirectional(GRU(128)))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "        model.add(Dense(output_dim, activation='sigmoid'))\n",
        "    elif model_type == 'cnn-gru': # 1D CNN - GRU\n",
        "        model.add(Embedding(num_words+1, 50, input_length=input_length))\n",
        "        model.add(Conv1D(192,3,activation='relu'))\n",
        "        model.add(Bidirectional(GRU(224, return_sequences=True)))\n",
        "        model.add(Bidirectional(GRU(384)))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "        model.add(Dense(output_dim, activation='sigmoid'))\n",
        "    elif model_type == 'cnn': # 1D CNN\n",
        "        model.add(Embedding(num_words+1, 50, input_length=input_length))\n",
        "        model.add(Conv1D(192, 10, activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv1D(192, 3, activation='relu'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "        model.add(Dense(output_dim, activation='sigmoid'))\n",
        "    else:\n",
        "        #raise ValueError(model_type + ' is not supported.')\n",
        "        print('wrong model')\n",
        " \n",
        "    model.summary()    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYeByESDQJIl"
      },
      "source": [
        "# def build_sequence_model(trainset, testset, model_type, num_words, input_length, output_dim=1, dropout_rate=0.0,\n",
        "#                      batch_size=32, nb_epochs=100, lr=0.001):\n",
        "#     \"\"\"Train and evaluate CNN model\n",
        "#     :param trainset: (X_train, y_train)\n",
        "#     :param testset: (X_test, y_test)\n",
        "#     :param model_type: str, can be 'cnn-gru', 'cnn', 'gru', 'lstm'\n",
        "#     :param num_words: int\n",
        "#     :param input_length: int\n",
        "#     :param output_dim: int\n",
        "#     :param batch_size: int, batch size for model training\n",
        "#     :param nb_epochs: int, number of training epoches\n",
        "#     :param lr: float, learning rate\n",
        "#     :param save_path: path to save model\n",
        "#     :return model: fitted Keras model\n",
        "#     :return scores: dict, scores on test set for the fitted Keras model\n",
        "#     \"\"\"\n",
        "    \n",
        "#     # Create model\n",
        "#     model = create_model(model_type=model_type, num_words=num_words, input_length=input_length, output_dim=output_dim,\n",
        "#                          dropout_rate=dropout_rate)\n",
        "    \n",
        "#     # Callback list\n",
        "#     callback_list = []\n",
        "#     # monitor val_loss and terminate training if no improvement\n",
        "#     early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, \\\n",
        "#                 patience=20, verbose=2, mode='auto', restore_best_weights=True)\n",
        "#     callback_list.append(early_stop)\n",
        "    \n",
        "#     # if save_path is not None:\n",
        "#     #     # save best model based on val_acc during training\n",
        "#     #     checkpoint = callbacks.ModelCheckpoint(os.path.join(save_path, '.h5'), monitor='val_acc', \\\n",
        "#     #                 verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
        "#     #     callback_list.append(checkpoint)\n",
        "        \n",
        "#     # Get train and test set\n",
        "#     (X_train, y_train) = trainset\n",
        "#     (X_test, y_test) = testset\n",
        "    \n",
        "#     # Compute class weights\n",
        "#     weight_list = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
        "#     weight_dict = {}\n",
        "#     for i in range(len(np.unique(y_train))):\n",
        "#         weight_dict[np.unique(y_train)[i]] = weight_list[i]\n",
        "    \n",
        "#     # Train only classification head\n",
        "#     optimizer = Adam(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "#     model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "#     model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=nb_epochs, \\\n",
        "#                         class_weight=weight_dict, callbacks=callback_list, verbose=2)\n",
        "    \n",
        "#     # Evaluate model    \n",
        "#     prediction = model.predict(X_test)\n",
        "#     y_val_predict = (prediction > 0.5).astype('uint8')\n",
        "#     with warnings.catch_warnings():\n",
        "#         warnings.simplefilter('ignore')  # disable the warning on f1-score with not all labels\n",
        "#         scores = get_prediction_score(y_val, y_val_predict)\n",
        "        \n",
        "#     return model, scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwXxmwLSZFuA"
      },
      "source": [
        "def build_sequence_model(trainset, testset, model_type, num_words, input_length, output_dim=1, dropout_rate=0.0,\n",
        "                     batch_size=32, nb_epochs=100, lr=0.001,\n",
        "                     save_path=None):\n",
        "    \"\"\"Train and evaluate CNN model\n",
        "    :param trainset: (X_train, y_train)\n",
        "    :param testset: (X_test, y_test)\n",
        "    :param model_type: str, can be 'cnn-gru', 'cnn', 'gru', 'lstm'\n",
        "    :param num_words: int\n",
        "    :param input_length: int\n",
        "    :param output_dim: int\n",
        "    :param batch_size: int, batch size for model training\n",
        "    :param nb_epochs: int, number of training epoches\n",
        "    :param lr: float, learning rate\n",
        "    :param save_path: path to save model\n",
        "    :return model: fitted Keras model\n",
        "    :return scores: dict, scores on test set for the fitted Keras model\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create model\n",
        "    model = create_model(model_type=model_type, num_words=num_words, input_length=input_length, output_dim=output_dim,\n",
        "                         dropout_rate=dropout_rate)\n",
        "    \n",
        "    # Callback list\n",
        "    callback_list = []\n",
        "    # monitor val_loss and terminate training if no improvement\n",
        "    early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, \\\n",
        "                patience=20, verbose=2, mode='auto', restore_best_weights=True)\n",
        "    callback_list.append(early_stop)\n",
        "    \n",
        "    if save_path is not None:\n",
        "        # save best model based on val_acc during training\n",
        "        checkpoint = callbacks.ModelCheckpoint(os.path.join(save_path, model_type + '.h5'), monitor='val_acc', \\\n",
        "                    verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
        "        callback_list.append(checkpoint)\n",
        "        \n",
        "    # Get train and test set\n",
        "    (X_train, y_train) = trainset\n",
        "    (X_test, y_test) = testset\n",
        "    \n",
        "    # Compute class weights\n",
        "    weight_list = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
        "    weight_dict = {}\n",
        "    for i in range(len(np.unique(y_train))):\n",
        "        weight_dict[np.unique(y_train)[i]] = weight_list[i]\n",
        "    \n",
        "    # Train only classification head\n",
        "    optimizer = Adam(lr=lr, decay=1e-6)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=nb_epochs, \\\n",
        "                        class_weight=weight_dict, callbacks=callback_list, verbose=2)\n",
        "    \n",
        "    # Evaluate model    \n",
        "    prediction = model.predict(X_test)\n",
        "    y_val_predict = (prediction > 0.5).astype('uint8')\n",
        "    y_val = y_test\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings('ignore')  # disable the warning on f1-score with not all labels\n",
        "        scores = get_prediction_score(y_val, y_val_predict)\n",
        "        \n",
        "    return model, scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nn2qOOkRtyg",
        "outputId": "00687163-f153-4559-a384-203d36001ff2"
      },
      "source": [
        "data_path = '/content/data/HIV.csv'\n",
        "\n",
        "\n",
        "model_list = ['cnn', 'cnn-gru', 'gru', 'lstm']\n",
        "batch_size = 16\n",
        "nb_epochs = 1\n",
        "lr = 0.001\n",
        "save_path = '/content/data'\n",
        "\n",
        "WORK_DIRECTORY = '/content/data'\n",
        "\n",
        "# Make save_path\n",
        "# if save_path is not None:\n",
        "#     os.makedirs(os.path.join(save_path, 'sequence_models'), exist_ok=True)\n",
        "\n",
        "# Read data\n",
        "smiles, y = read_data(data_path, col_smiles='smiles', col_target='HIV_active')\n",
        "tokens, num_words, max_phrase_len = generate_tokens(smiles, len_percentile=100)\n",
        "\n",
        "# Get train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(tokens, y, test_size=TEST_RATIO, shuffle=True, stratify=y,\n",
        "                                                  random_state=SEED)\n",
        "\n",
        "# Build en evaluate graph models\n",
        "model_scores = []\n",
        "for model_type in model_list:\n",
        "    model, scores = build_sequence_model((X_train, y_train), (X_test, y_test), model_type, num_words, max_phrase_len,\n",
        "                                          output_dim=1, dropout_rate=0.0,\n",
        "                                          batch_size=batch_size, nb_epochs=nb_epochs, lr=lr,\n",
        "                             save_path=os.path.join(save_path, 'sequence_models', model_type + '.h5'))\n",
        "    model_scores.append(scores)\n",
        "        \n",
        "    # force release memory\n",
        "    K.clear_session()\n",
        "    del model\n",
        "    gc.collect()\n",
        "\n",
        "model_df = pd.DataFrame({'model': model_list,\n",
        "\n",
        "                    METRIC_ACCURACY: [score[METRIC_ACCURACY] for score in model_scores],\n",
        "                    METRIC_F1_SCORE: [score[METRIC_F1_SCORE] for score in model_scores],\n",
        "                    METRIC_COHEN_KAPPA: [score[METRIC_COHEN_KAPPA] for score in model_scores],\n",
        "                    METRIC_CONFUSION_MATRIX: [score[METRIC_CONFUSION_MATRIX] for score in model_scores]                            \n",
        "                      })\n",
        "model_df = model_df[['model', METRIC_ACCURACY, METRIC_F1_SCORE, METRIC_COHEN_KAPPA,\n",
        "                      METRIC_CONFUSION_MATRIX]]\n",
        "\n",
        "\n",
        "model_df.to_csv(os.path.join(WORK_DIRECTORY, 'summary_sequence_model.csv'), index=False)\n",
        "model_df.sort_values(by=[METRIC_ACCURACY, METRIC_F1_SCORE, METRIC_COHEN_KAPPA],\n",
        "                      ascending=False, inplace=True)\n",
        "print('Best model:\\n' + str(model_df.iloc[0]))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True max length is 580, 580 is set the length cutoff.\n",
            "Vocab size is 56\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 580, 50)           2850      \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 571, 192)          96192     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 571, 192)          768       \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 569, 192)          110784    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 109248)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               13983872  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 14,194,595\n",
            "Trainable params: 14,194,211\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "1029/1029 - 18s - loss: 0.7192 - accuracy: 0.7575 - val_loss: 0.7550 - val_accuracy: 0.6839\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 580, 50)           2850      \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 578, 192)          28992     \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 578, 448)          561792    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 768)               1921536   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               98432     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,613,731\n",
            "Trainable params: 2,613,731\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1029/1029 - 192s - loss: 0.7056 - accuracy: 0.3726 - val_loss: 0.6964 - val_accuracy: 0.0351\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 580, 50)           2850      \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 580, 256)          138240    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 256)               296448    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 470,563\n",
            "Trainable params: 470,563\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1029/1029 - 103s - loss: 0.6959 - accuracy: 0.5327 - val_loss: 0.7033 - val_accuracy: 0.0675\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 580, 50)           2850      \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 580, 256)          183296    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 256)               394240    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 613,411\n",
            "Trainable params: 613,411\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1029/1029 - 106s - loss: 0.6948 - accuracy: 0.3282 - val_loss: 0.6957 - val_accuracy: 0.2684\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Best model:\n",
            "model                                      cnn\n",
            "accuracy                              0.683929\n",
            "f1-score                              0.464175\n",
            "Cohen kappa                          0.0615286\n",
            "Confusion Matrix    [[5447, 2490], [110, 179]]\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrPA3iDmsFvw",
        "outputId": "2c684b5b-0a94-4baa-fb8e-886de1b62e5e"
      },
      "source": [
        "print(model_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     model  accuracy  f1-score  Cohen kappa            Confusion Matrix\n",
            "0      cnn  0.683929  0.464175     0.061529  [[5447, 2490], [110, 179]]\n",
            "3     lstm  0.268417  0.232817     0.000468   [[1990, 5947], [71, 218]]\n",
            "2      gru  0.067469  0.067467     0.000981     [[272, 7665], [6, 283]]\n",
            "1  cnn-gru  0.035133  0.033940     0.000000       [[0, 7937], [0, 289]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}