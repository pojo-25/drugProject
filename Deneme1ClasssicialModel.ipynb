{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deneme1ClasssicialModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOypfkVN3jEijdZ2YlX71Yg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pojo-25/drugProject/blob/main/Deneme1ClasssicialModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLkTBRI_KaaL"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.insert(0, os.getcwd()) # add current working directory to pythonpath\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import linear_model\n",
        "import pickle\n",
        "import warnings\n",
        "import argparse"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ1IdrwctKb3"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, confusion_matrix"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIqtvrRwTzFB",
        "outputId": "4c6ae695-d806-414b-ec80-856dea973604"
      },
      "source": [
        "url = 'https://anaconda.org/rdkit/rdkit/2018.09.1.0/download/linux-64/rdkit-2018.09.1.0-py36h71b666b_1.tar.bz2'\n",
        "!curl -L $url | tar xj lib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3773    0  3773    0     0   8440      0 --:--:-- --:--:-- --:--:--  8421\n",
            "100 20.2M  100 20.2M    0     0  5469k      0  0:00:03  0:00:03 --:--:-- 7646k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpJcABPET7m4"
      },
      "source": [
        "# move to python packages directory\n",
        "!mv lib/python3.6/site-packages/rdkit /usr/local/lib/python3.6/dist-packages/\n",
        "x86 = '/usr/lib/x86_64-linux-gnu'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UtPDCljUAFZ"
      },
      "source": [
        "!mv lib/*.so.* $x86/"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_M32TFCUFZQ"
      },
      "source": [
        "# rdkit need libboost_python3.so.1.65.1\n",
        "!ln -s $x86/libboost_python3-py36.so.1.65.1 $x86/libboost_python3.so.1.65.1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEznTdQMXN8L",
        "outputId": "b356bc42-9b3d-4f08-9507-8be5c6298b4a"
      },
      "source": [
        "!pip install kora -q\n",
        "import kora.install.rdkit"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▊                          | 10kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20kB 19.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.7MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |██████                          | 10kB 29.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 20kB 37.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 30kB 37.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 40kB 39.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 51kB 41.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 8.8MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN3mlglJRbSP"
      },
      "source": [
        "from sklearn.feature_selection import chi2\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdMolDescriptors, AllChem, ChemicalFeatures, Descriptors, Crippen, Lipinski\n",
        "from rdkit.Chem.Pharm2D import Generate, Gobbi_Pharm2D\n",
        "from rdkit.Chem.Pharm2D.SigFactory import SigFactory\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2xUvyfHSw2z"
      },
      "source": [
        "fdefName = '/content/models/MinimalFeatures.fdef'\n",
        "featFactory = ChemicalFeatures.BuildFeatureFactory(fdefName)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwWrnYxiYNpD"
      },
      "source": [
        "def extract_properties(column, include_3D=False):\n",
        "    \"\"\"Extract various 1D descriptors\n",
        "    https://www.rdkit.org/docs/GettingStartedInPython.html#list-of-available-descriptors\n",
        "    \n",
        "    :param column: Pandas Series, containing smiles or RDKit mol object\n",
        "    :param from_smiles: bool, indicate whether column contains smiles string\n",
        "    :return: feature_properties: Pandas Series, containing 1D descriptors\n",
        "    \"\"\"\n",
        "\n",
        "    def extract(x, from_smiles):\n",
        "        if from_smiles:\n",
        "            mol = Chem.MolFromSmiles(x)\n",
        "        else:\n",
        "            mol = x\n",
        "        \n",
        "        if (mol is None) or (len(mol.GetAtoms()) == 0):\n",
        "            if include_3D:\n",
        "                return [0] * 29\n",
        "            else:\n",
        "                return [0] * 24\n",
        "        else:\n",
        "            logP = Crippen.MolLogP(mol)\n",
        "            refractivity = Crippen.MolMR(mol)\n",
        "            \n",
        "            weight = Descriptors.MolWt(mol)\n",
        "            exact_weight = Descriptors.ExactMolWt(mol)\n",
        "            heavy_weight = Descriptors.HeavyAtomMolWt(mol)\n",
        "            heavy_count = Lipinski.HeavyAtomCount(mol)\n",
        "            nhoh_count = Lipinski.NHOHCount(mol)\n",
        "            no_count = Lipinski.NOCount(mol)\n",
        "            hacceptor_count = Lipinski.NumHAcceptors(mol)\n",
        "            hdonor_count = Lipinski.NumHDonors(mol)\n",
        "            hetero_count = Lipinski.NumHeteroatoms(mol)\n",
        "            rotatable_bond_count = Lipinski.NumRotatableBonds(mol)\n",
        "            valance_electron_count = Descriptors.NumValenceElectrons(mol)\n",
        "            amide_bond_count = rdMolDescriptors.CalcNumAmideBonds(mol)\n",
        "            aliphatic_ring_count = Lipinski.NumAliphaticRings(mol)\n",
        "            aromatic_ring_count = Lipinski.NumAromaticRings(mol)\n",
        "            saturated_ring_count = Lipinski.NumSaturatedRings(mol)\n",
        "            aliphatic_cycle_count = Lipinski.NumAliphaticCarbocycles(mol)\n",
        "            aliphaticHetero_cycle_count = Lipinski.NumAliphaticHeterocycles(mol)\n",
        "            aromatic_cycle_count = Lipinski.NumAromaticCarbocycles(mol)\n",
        "            aromaticHetero_cycle_count = Lipinski.NumAromaticHeterocycles(mol)\n",
        "            saturated_cycle_count = Lipinski.NumSaturatedCarbocycles(mol)\n",
        "            saturatedHetero_cycle_count = Lipinski.NumSaturatedHeterocycles(mol)\n",
        "            \n",
        "            tpsa = rdMolDescriptors.CalcTPSA(mol)\n",
        "            \n",
        "            if include_3D:\n",
        "                mol_3D=Chem.AddHs(mol)\n",
        "                AllChem.EmbedMolecule(mol_3D)\n",
        "                AllChem.MMFFOptimizeMolecule(mol_3D)\n",
        "                eccentricity = rdMolDescriptors.CalcEccentricity(mol_3D)\n",
        "                asphericity = rdMolDescriptors.CalcAsphericity(mol_3D)\n",
        "                spherocity = rdMolDescriptors.CalcSpherocityIndex(mol_3D)\n",
        "                inertial = rdMolDescriptors.CalcInertialShapeFactor(mol_3D)\n",
        "                gyration = rdMolDescriptors.CalcRadiusOfGyration(mol_3D)\n",
        "            \n",
        "                return [logP, refractivity, weight, exact_weight, heavy_weight, heavy_count, nhoh_count, no_count,\n",
        "                        hacceptor_count, hdonor_count, hetero_count, rotatable_bond_count, valance_electron_count,\n",
        "                        amide_bond_count, aliphatic_ring_count, aromatic_ring_count, saturated_ring_count,\n",
        "                        aliphatic_cycle_count, aliphaticHetero_cycle_count, aromatic_cycle_count,\n",
        "                        aromaticHetero_cycle_count, saturated_cycle_count, saturatedHetero_cycle_count, tpsa,\n",
        "                        eccentricity, asphericity, spherocity, inertial, gyration]\n",
        "            else:\n",
        "                return [logP, refractivity, weight, exact_weight, heavy_weight, heavy_count, nhoh_count, no_count,\n",
        "                        hacceptor_count, hdonor_count, hetero_count, rotatable_bond_count, valance_electron_count,\n",
        "                        amide_bond_count, aliphatic_ring_count, aromatic_ring_count, saturated_ring_count,\n",
        "                        aliphatic_cycle_count, aliphaticHetero_cycle_count, aromatic_cycle_count,\n",
        "                        aromaticHetero_cycle_count, saturated_cycle_count, saturatedHetero_cycle_count, tpsa]\n",
        "                \n",
        "    feature_properties = column.apply(lambda x: extract(x))\n",
        "\n",
        "    return np.array(list(feature_properties))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wz_kSIYYPJl"
      },
      "source": [
        "####################\n",
        "## 2D descriptors ##\n",
        "####################\n",
        "def extract_MQNs(column, from_smiles=True):\n",
        "    \"\"\"Extract MQN features from smiles\n",
        "    :param column: Pandas Series, containing smiles or RDKit mol object\n",
        "    :param from_smiles: bool, indicate whether column contains smiles string\n",
        "    :return feature_MQN: Pandas Series, containing 42 MQN feature\n",
        "    \"\"\"\n",
        "    \n",
        "    def get_MQNs(x, from_smiles):\n",
        "        if from_smiles:\n",
        "            mol = Chem.MolFromSmiles(x)\n",
        "        else:\n",
        "            mol = x\n",
        "        if (mol is None) or (len(mol.GetAtoms()) == 0):\n",
        "            return [0]*42\n",
        "        else:\n",
        "           return rdMolDescriptors.MQNs_(mol) \n",
        "       \n",
        "    feature_MQN = column.apply(lambda x: get_MQNs(x, from_smiles))\n",
        "    return np.array(list(feature_MQN))\n",
        "\n",
        "\n",
        "def extract_Morganfp(column, radius=2, nBits=2048, useFeatures=False, from_smiles=True):\n",
        "    \"\"\"Extract Morganfingerprint\n",
        "    :param column: Pandas Series, containing smiles or RDKit mol object\n",
        "    :param radius: int, indicates the radius in the Morgan fingerprint calculation.\n",
        "    :param nBits: int, the number of bits in the resulting bit vector.\n",
        "    :param useFeatures: bool, whether atoms' specific features are used\n",
        "    :param from_smiles: bool, indicate whether column contains smiles string\n",
        "    :return: feature_morgan: Pandas Series, containing Morganfingerprint features\n",
        "    \"\"\"\n",
        "    \n",
        "    def get_Morganfp(x, from_smiles):\n",
        "        if from_smiles:\n",
        "            mol = Chem.MolFromSmiles(x)\n",
        "        else:\n",
        "            mol = x\n",
        "        if (mol is None) or (len(mol.GetAtoms()) == 0):\n",
        "            return [0]*nBits\n",
        "        else:\n",
        "           return AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits, useFeatures=useFeatures) \n",
        "        \n",
        "    feature_morgan = column.apply(lambda x: get_Morganfp(x, from_smiles))\n",
        "    return np.array(list(feature_morgan))\n",
        "\n",
        "\n",
        "def extract_Pharm2D(column, minPointCount=2, maxPointCount=3, bins=[(0,2),(2,5),(5,8)], from_smiles=True):\n",
        "    \"\"\"Extract Pharm2D fingerprint\n",
        "    :param column: Pandas Series, containing smiles or RDKit mol object\n",
        "    :param minPointCount: int\n",
        "    :param maxPointCount: int\n",
        "    :param bins: lits of tuples\n",
        "    :param from_smiles: bool, indicate whether column contains smiles string\n",
        "    :return: feature_Pharm2D: Pandas Series, containing Pharm2D features\n",
        "    \"\"\"\n",
        "    sigFactory = SigFactory(featFactory,\n",
        "                            minPointCount=minPointCount,\n",
        "                            maxPointCount=maxPointCount,\n",
        "                            trianglePruneBins=False)\n",
        "    sigFactory.SetBins(bins)\n",
        "    sigFactory.Init()\n",
        "    \n",
        "    def get_Pharm2D(x):\n",
        "        mol = Chem.MolFromSmiles(x)\n",
        "        if (mol is None) or (len(mol.GetAtoms()) == 0):\n",
        "            return [0]*sigFactory.GetSigSize()\n",
        "        else:\n",
        "           return Generate.Gen2DFingerprint(mol, sigFactory)     \n",
        "\n",
        "    fp = column.apply(lambda x: get_Pharm2D(x))\n",
        "    return np.array(list(fp))\n",
        "\n",
        "\n",
        "def extract_Gobbi_Pharm2D(column, from_smiles=True):\n",
        "    \"\"\"Extract Gobbi Pharm2D fingerprint\n",
        "    :param column: Pandas Series, containing smiles or RDKit mol object\n",
        "    :param from_smiles: bool, indicate whether column contains smiles string\n",
        "    :return: feature_Gobbi Pharm2D: Pandas Series, containing Gobbi Pharm2D  features\n",
        "    \"\"\"\n",
        "    \n",
        "    def get_Gobbi_Pharm2D(x, from_smiles):\n",
        "        if from_smiles:\n",
        "            mol = Chem.MolFromSmiles(x)\n",
        "        else:\n",
        "            mol = x\n",
        "        return Generate.Gen2DFingerprint(mol, Gobbi_Pharm2D.factory)\n",
        "        \n",
        "    feature_Gobbi = column.apply(lambda x: get_Gobbi_Pharm2D(x, from_smiles))\n",
        "    return np.array(list(feature_Gobbi))\n",
        "\n",
        "\n",
        "####################\n",
        "## 3D descriptors ##\n",
        "####################\n",
        "def extract_RDF(column, from_smiles=True):\n",
        "    \"\"\"Extract RDF descriptor\n",
        "    :param column: Pandas Series, containing smiles or RDKit mol object\n",
        "    :param from_smiles: bool, indicate whether column contains smiles string\n",
        "    :return: feature_RDF: Pandas Series, containing 210 RDF features\n",
        "    \"\"\"\n",
        "    def get_RDF(x, from_smiles):\n",
        "        if from_smiles:\n",
        "            mol = Chem.MolFromSmiles(x)\n",
        "        else:\n",
        "            mol = x\n",
        "        if (mol is None) or (len(mol.GetAtoms()) == 0):\n",
        "            return [0]*210\n",
        "        else:\n",
        "            mol_3D=Chem.AddHs(mol)\n",
        "            AllChem.EmbedMolecule(mol_3D)\n",
        "            AllChem.MMFFOptimizeMolecule(mol_3D)\n",
        "            return rdMolDescriptors.CalcRDF(mol_3D) \n",
        "        \n",
        "    feature_RDF = column.apply(lambda x: get_RDF(x, from_smiles))\n",
        "    return np.array(list(feature_RDF))\n",
        "\n",
        "\n",
        "def extract_AUTOCORR3D(column, from_smiles=True):\n",
        "    \"\"\"Extract AUTOCORR3D descriptor\n",
        "    :param column: Pandas Series, containing smiles or RDKit mol object\n",
        "    :param from_smiles: bool, indicate whether column contains smiles string\n",
        "    :return: feature_AUTOCORR3D: Pandas Series, containing 80 AUTOCORR3D features\n",
        "    \"\"\"\n",
        "    def get_AUTOCORR3D(x, from_smiles):\n",
        "        if from_smiles:\n",
        "            mol = Chem.MolFromSmiles(x)\n",
        "        else:\n",
        "            mol = x\n",
        "        if (mol is None) or (len(mol.GetAtoms()) == 0):\n",
        "            return [0]*80\n",
        "        else:\n",
        "            mol_3D=Chem.AddHs(mol)\n",
        "            AllChem.EmbedMolecule(mol_3D)\n",
        "            AllChem.MMFFOptimizeMolecule(mol_3D)\n",
        "            return rdMolDescriptors.CalcAUTOCORR3D(mol_3D) \n",
        "        \n",
        "    feature_AUTOCORR3D = column.apply(lambda x: get_AUTOCORR3D(x, from_smiles))\n",
        "    return np.array(list(feature_AUTOCORR3D)), np.arange(80)\n",
        "\n",
        "\n",
        "def extract_MORSE(column, from_smiles=True):\n",
        "    \"\"\"Extract MORSE descriptor\n",
        "    :param column: Pandas Series, containing smiles or RDKit mol object\n",
        "    :param from_smiles: bool, indicate whether column contains smiles string\n",
        "    :return: feature_MORSE: Pandas Series, containing 224 MORSE features\n",
        "    \"\"\"\n",
        "    def get_MORSE(x, from_smiles):\n",
        "        if from_smiles:\n",
        "            mol = Chem.MolFromSmiles(x)\n",
        "        else:\n",
        "            mol = x\n",
        "        if (mol is None) or (len(mol.GetAtoms()) == 0):\n",
        "            return [0]*224\n",
        "        else:\n",
        "            mol_3D=Chem.AddHs(mol)\n",
        "            AllChem.EmbedMolecule(mol_3D)\n",
        "            AllChem.MMFFOptimizeMolecule(mol_3D)\n",
        "            return rdMolDescriptors.CalcMORSE(mol_3D) \n",
        "        \n",
        "    feature = column.apply(lambda x: get_MORSE(x, from_smiles))\n",
        "    return np.array(list(feature)), np.arange(224)\n",
        "\n",
        "\n",
        "def extract_WHIM(column, from_smiles=True):\n",
        "    \"\"\"Extract WHIM descriptor\n",
        "    :param column: Pandas Series, containing smiles or RDKit mol object\n",
        "    :param from_smiles: bool, indicate whether column contains smiles string\n",
        "    :return: feature_WHIM: Pandas Series, containing 114 WHIM features\n",
        "    \"\"\"\n",
        "    def get_WHIM(x, from_smiles):\n",
        "        if from_smiles:\n",
        "            mol = Chem.MolFromSmiles(x)\n",
        "        else:\n",
        "            mol = x\n",
        "        if (mol is None) or (len(mol.GetAtoms()) == 0):\n",
        "            return [0]*114\n",
        "        else:\n",
        "            mol_3D=Chem.AddHs(mol)\n",
        "            AllChem.EmbedMolecule(mol_3D)\n",
        "            AllChem.MMFFOptimizeMolecule(mol_3D)\n",
        "            return rdMolDescriptors.CalcWHIM(mol_3D) \n",
        "        \n",
        "    feature = column.apply(lambda x: get_WHIM(x, from_smiles))\n",
        "    return np.array(list(feature)), np.arange(114)\n",
        "\n",
        "\n",
        "def extract_GETAWAY(column, from_smiles=True):\n",
        "    \"\"\"Extract GETAWAY descriptor. GETAWAT descriptors have NaN values sometimes.\n",
        "    :param column: Pandas Series, containing smiles or RDKit mol object\n",
        "    :param from_smiles: bool, indicate whether column contains smiles string\n",
        "    :return: feature_GETAWAY: Pandas Series, containing 273 GETAWAY features\n",
        "    \"\"\"\n",
        "    def get_GETAWAY(x):\n",
        "        mol = Chem.MolFromSmiles(x)\n",
        "        if (mol is None) or (len(mol.GetAtoms()) == 0):\n",
        "            return [0]*273\n",
        "        else:\n",
        "            mol_3D=Chem.AddHs(mol)\n",
        "            AllChem.EmbedMolecule(mol_3D)\n",
        "            AllChem.MMFFOptimizeMolecule(mol_3D)\n",
        "            return rdMolDescriptors.CalcGETAWAY(mol_3D) \n",
        "        \n",
        "    feature = column.apply(lambda x: get_GETAWAY(x))\n",
        "    return np.array(list(feature)), np.arange(273)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le-Xjo7YYVob"
      },
      "source": [
        "def extract_features(column, method=['morgan'], from_smiles=True):\n",
        "    \"\"\"Extract 1D, 2D and 3D descriptors\n",
        "    :param column: Pandas Series, containing smiles or RDKit mol object\n",
        "    :param method: list, containing names of descriptors to extract\n",
        "    :param from_smiles: bool, indicate whether column contains smiles string\n",
        "    :return features: list of features extracted with method list\n",
        "    \"\"\"\n",
        "\n",
        "    feature_list = []\n",
        "    if 'morgan' in method:\n",
        "        feature_list.append(extract_Morganfp(column))\n",
        "    if 'mqn' in method:\n",
        "        feature_list.append(extract_MQNs(column))\n",
        "    if 'pharm2D' in method:\n",
        "        feature_list.append(extract_Pharm2D(column))\n",
        "    if 'gobbi' in method:\n",
        "        feature_list.append(extract_Gobbi_Pharm2D(column))\n",
        "    if 'physical' in method:\n",
        "        feature_list.append(extract_properties(column, include_3D=False))\n",
        "    if 'physical3D' in method:\n",
        "        feature_list.append(extract_properties(column, include_3D=True))\n",
        "    if 'autocorr3D' in method:\n",
        "        feature_list.append(extract_AUTOCORR3D(column))\n",
        "    if 'rdf' in method:\n",
        "        feature_list.append(extract_RDF(column))\n",
        "    if 'morse' in method:\n",
        "        feature_list.append(extract_MORSE(column))\n",
        "    if 'whim' in method:\n",
        "        feature_list.append(extract_WHIM(column))\n",
        "    if 'getaway' in method:\n",
        "        feature_list.append(extract_GETAWAY(column))\n",
        "\n",
        "    return np.concatenate(feature_list, axis=1)\n",
        "        \n",
        "     \n",
        "def filter_feature_Chi2(feature_column, target_column, threshold=None):\n",
        "    \"\"\"Filter feature using Chi2 test\n",
        "    :param: feature_column: numpy array containing feature\n",
        "    :param: target_column: numpy array containing target variable\n",
        "    :param: threshold: threshold to filter using Chi2.\n",
        "                        threshold=None means all features with non Nan Chi2 pval will be returned.\n",
        "    :return: feature_selected: features that are signficant\n",
        "    :return: pval_significant: list of bool to indicate which features are significant\n",
        "    \"\"\"\n",
        "\n",
        "    # Perform Chi2 test\n",
        "    chi2_stats, pval = chi2(feature_column, target_column)\n",
        "\n",
        "    # select only significant pvals\n",
        "    pval_result = pval.copy()\n",
        "    if threshold:\n",
        "        pval_result[np.isnan(pval_result)] = 100  # replace Nan with any large value\n",
        "        pval_significant = pval_result <= threshold\n",
        "    else:\n",
        "        pval_significant = np.logical_not(np.isnan(pval))\n",
        "\n",
        "    # select features with significant pvals\n",
        "    feature_selected = feature_column[:, pval_significant]\n",
        "\n",
        "    return feature_selected, pval_significant"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAx_V9DGYxaj"
      },
      "source": [
        "col_smiles = 'smiles'\n",
        "col_target = 'HIV_active'\n",
        "\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "METRIC_F1_SCORE = 'f1-score'\n",
        "METRIC_COHEN_KAPPA = 'Cohen kappa'\n",
        "METRIC_CONFUSION_MATRIX = 'Confusion Matrix'\n",
        "\n",
        "\n",
        "CLASSES = ['benign', 'malignant']\n",
        "TEST_RATIO = 0.2\n",
        "SEED = 0\n",
        "\n",
        "data_path = '/content/data/HIV.csv'"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHMmCd5BYoWr"
      },
      "source": [
        "def read_data(data_path, col_smiles='smiles', col_target='HIV_active'):\n",
        "    \"\"\"Split original data into train data and test data.\n",
        "    :param data_path: str, path to the a CSV data file\n",
        "    :param col_smiles: str, name of smiles column\n",
        "    :param col_target: str, name of target column\n",
        "    :param test_ratio: float, proportion of the original data for testset, must be from 0 to 1\n",
        "    :param seed: int, randomization seed for reproducibility\n",
        "    :return (X, y)\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    # read data\n",
        "    df = pd.read_csv(data_path, sep=',')\n",
        "    df_no_na = df[[col_smiles, col_target]].dropna()\n",
        "\n",
        "    X = df_no_na[col_smiles]\n",
        "    y = df_no_na[col_target].values\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "                \n",
        "def get_prediction_score(y_label, y_predict):\n",
        "    \"\"\"Evaluate predictions using different evaluation metrics.\n",
        "    :param y_label: list, contains true label\n",
        "    :param y_predict: list, contains predicted label\n",
        "    :return scores: dict, evaluation metrics on the prediction\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "    scores[METRIC_ACCURACY] = accuracy_score(y_label, y_predict)\n",
        "    scores[METRIC_F1_SCORE] = f1_score(y_label, y_predict, labels=None, average='macro', sample_weight=None)\n",
        "    scores[METRIC_COHEN_KAPPA] = cohen_kappa_score(y_label, y_predict)\n",
        "    scores[METRIC_CONFUSION_MATRIX] = confusion_matrix(y_label, y_predict)\n",
        "    \n",
        "    return scores"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lw8N0nwYbyL"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.insert(0, os.getcwd()) # add current working directory to pythonpath\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import linear_model\n",
        "import pickle\n",
        "import warnings\n",
        "import argparse"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gznEWrCBO0WL"
      },
      "source": [
        "def train_model(model, X_train, y_train, parameters, n_splits=3):\n",
        "    \"\"\"Train model with Grid-search cross validation to find the best hyperparameter\n",
        "    :param model: Scikit-learn estimator\n",
        "    :param X_train: trainset features\n",
        "    :param y_train: trainset label\n",
        "    :param parameters: dict, key is hyper parameter name and value is a list of hyper parameter values\n",
        "    :return best_estimator: Scikit-learn estimator with the best hyper parameter\n",
        "    :return best_score: best accuracy score\n",
        "    :return best_param: dict, best hyper parameter\n",
        "    \"\"\"\n",
        "    \n",
        "    splits = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0).split(X=X_train, y=y_train)\n",
        "    \n",
        "    clf = GridSearchCV(model, parameters, cv=splits, scoring=make_scorer(accuracy_score))\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter('ignore')  # disable the warning on default optimizer\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "    return clf.best_estimator_, clf.best_score_, clf.best_params_\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Evaluate model on testset\n",
        "    :param model: Scikit-learn estimator\n",
        "    :param X_train: trainset features\n",
        "    :param y_train: trainset label\n",
        "    :param X_test: testset features\n",
        "    :param y_test: testset label\n",
        "    :param parameters: dict, key is hyper parameter name and value is a list of hyper parameter values\n",
        "    :return model: Scikit-learn estimator, fitted on the whole trainset\n",
        "    :return y_predict: prediction on test set\n",
        "    :return scores: dict, evaluation metrics on test set\n",
        "    \"\"\"\n",
        "    \n",
        "    # Refit the model on the whole train set\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter('ignore')  # disable the warning on default optimizer\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "    # Evaluate on test set\n",
        "    y_predict = model.predict(X_test)\n",
        "    scores = None\n",
        "    if y_test is not None:\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter('ignore')  # disable the warning on f1-score with not all labels\n",
        "            scores = get_prediction_score(y_test, y_predict)\n",
        "\n",
        "    return model, y_predict, scores\n",
        "\n",
        "\n",
        "def build_base_models(X_train, y_train):\n",
        "    \"\"\"Train and evaluate different base models. \"Base\" means the model is not a stacking model. \n",
        "    :param X_train: trainset features\n",
        "    :param y_train: trainset label\n",
        "    :return fitted_models: list, contains fitted Scikit-learn estimators\n",
        "    :return model_names: list, names of fitted Scikit-learn estimators\n",
        "    :return model_scores: list, contains scores on test set for fitted Scikit-learn estimators.\n",
        "                    Each score is a dict of evaluation metrics\n",
        "    \"\"\"\n",
        "    \n",
        "    ########################\n",
        "    # DEFINE BASE MODELS ###\n",
        "    ########################\n",
        "    models = []\n",
        "    model_params = []\n",
        "    model_names = []\n",
        "    \n",
        "    # Random forest model\n",
        "    for n_estimators in [500, 1000, 2000]:\n",
        "        for max_depth in [3, 5, 7]:\n",
        "            models.append(RandomForestClassifier(max_features='sqrt', class_weight='balanced', random_state=0))\n",
        "            model_params.append({'n_estimators':[n_estimators], 'max_depth':[max_depth]})\n",
        "            model_names.append('Random Forest')   \n",
        "    \n",
        "    # Boosted Tree\n",
        "    for n_estimators in [500, 1000, 2000]:\n",
        "        for max_depth in [3, 5, 7]:\n",
        "            for learning_rate in [0.01, 0.1]:\n",
        "                models.append(GradientBoostingClassifier(subsample=0.7, max_features='sqrt', random_state=0))\n",
        "                model_params.append({'n_estimators':[n_estimators], 'max_depth':[max_depth], 'learning_rate':[learning_rate]})\n",
        "                model_names.append('Gradient Boosting Machine')\n",
        "    \n",
        "    # SVM\n",
        "#    for kernel in ['linear', 'rbf']:\n",
        "#        for C in [1.0, 10.0, 100.0, 1000.0]:\n",
        "#            models.append(SVC(probability=True, gamma='auto', tol=0.001, cache_size=200, class_weight='balanced',\n",
        "#                              random_state=0,\n",
        "#                              decision_function_shape='ovr'))\n",
        "#            model_params.append({'kernel':[kernel], 'C':[C]})\n",
        "#            model_names.append('Support Vector Machine')\n",
        "    \n",
        "    # Logistic regression model\n",
        "    for penalty in ['l1', 'l2']:\n",
        "        for C in [1.0, 10.0, 100.0, 1000.0]:\n",
        "            models.append(linear_model.LogisticRegression(max_iter=500, solver='liblinear', multi_class='ovr',\n",
        "                                                          class_weight='balanced', random_state=0))\n",
        "            model_params.append({'penalty':[penalty], 'C':[C]})\n",
        "            model_names.append('Logistic Regression')\n",
        "        \n",
        "    # KNN\n",
        "#    for n_neighbors in [5, 10, 15]:\n",
        "#        for weights in ['uniform', 'distance']:\n",
        "#            models.append(KNeighborsClassifier())\n",
        "#            model_params.append({'n_neighbors':[n_neighbors], 'weights':[weights]})\n",
        "#            model_names.append('K Nearest Neighbour')\n",
        "            \n",
        "    ##################################\n",
        "    # TRAIN AND EVALUATE BASE MODELS #\n",
        "    ##################################\n",
        "    fitted_models = []\n",
        "    model_scores = []\n",
        "    for i in range(len(models)):\n",
        "        print('Evaluating model {} of {}: {}'.format((i+1), len(models), model_names[i]))\n",
        "        model = models[i]\n",
        "        fitted_cv, _, _ = train_model(model=model, X_train=X_train, y_train=y_train, parameters=model_params[i])\n",
        "        fitted_whole_set, _, score = evaluate_model(model=fitted_cv, X_train=X_train, y_train=y_train,\n",
        "                                                    X_test=X_test, y_test=y_test)\n",
        "        fitted_models.append(fitted_whole_set)\n",
        "        model_scores.append(score)\n",
        "        print(model_names[i], score)\n",
        "        \n",
        "    return fitted_models, model_names, model_scores\n",
        "\n",
        "\n",
        "def build_stack_models(base_models, X_train, y_train):\n",
        "    \"\"\"Train and evaluate different stack models\n",
        "    :param base_models: list, contains fitted base models, which are Scikit-learn estimators\n",
        "    :param X_train: trainset features\n",
        "    :param y_train: trainset label\n",
        "    :return stack_fitted_models: list, contains fitted Scikit-learn estimators\n",
        "    :return stack_model_names: list, names of fitted Scikit-learn estimators\n",
        "    :return stack_model_scores: list, contains scores on test set for fitted Scikit-learn estimators.\n",
        "                    Each score is a dict of evaluation metrics\n",
        "    \"\"\"\n",
        "    \n",
        "    ###############################\n",
        "    ### PREPARE DATA FOR STACKING #\n",
        "    ###############################\n",
        "    print('Preparing data for model stacking')    \n",
        "    # Get base models' prediction for test set: simply use the trained models to predict on test set\n",
        "    X_test_stack = np.zeros([X_test.shape[0], len(base_models)])\n",
        "    for i in range(len(base_models)):\n",
        "        model = base_models[i]\n",
        "        X_test_stack[:, i] = model.predict(X_test)\n",
        "            \n",
        "    # Get base models' prediction for train set: use 3-fold split, train model on 2 parts and predict on 3rd part\n",
        "    splits = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0).split(X=X_train, y=y_train)\n",
        "    X_train_stack = np.zeros([X_train.shape[0], len(base_models)])\n",
        "    for train_index, val_index in splits:\n",
        "        # train and validation set\n",
        "        X_tr, X_val = X_train[train_index], X_train[val_index]\n",
        "        y_tr, _ = y_train[train_index], y_train[val_index]\n",
        "\n",
        "        # Fit model\n",
        "        for i in range(len(base_models)):\n",
        "            model = base_models[i]\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter('ignore')  # disable the warning on default optimizer\n",
        "                model.fit(X_tr, y_tr)\n",
        "            X_train_stack[val_index, i] = model.predict(X_val)\n",
        "\n",
        "    # Add base models' predictions into the feature space\n",
        "    X_train_stack = np.concatenate([X_train, X_train_stack], axis=-1)\n",
        "    X_test_stack = np.concatenate([X_test, X_test_stack], axis=-1)\n",
        "          \n",
        "    ########################\n",
        "    # DEFINE STACK MODELS ##\n",
        "    ########################\n",
        "    stack_models = []\n",
        "    stack_model_names = []\n",
        "    stack_model_params = []\n",
        "    \n",
        "    stack_models.append(linear_model.LogisticRegression(max_iter=500, solver='liblinear', multi_class='ovr',\n",
        "                                                        class_weight='balanced', random_state=0))\n",
        "    stack_model_names.append('Stack Logistic Regression')\n",
        "    stack_model_params.append({'penalty':['l1', 'l2'], 'C':[1.0, 10.0, 100.0, 1000.0]})\n",
        "    \n",
        "    stack_models.append(RandomForestClassifier(class_weight='balanced', random_state=0))\n",
        "    stack_model_names.append('Stack Random Forest')\n",
        "    stack_model_params.append({'n_estimators':[500, 1000, 2000], 'max_depth':[3, 5, 7]})\n",
        "    \n",
        "    stack_models.append(GradientBoostingClassifier(subsample=0.7, max_features='sqrt', learning_rate=0.01,\n",
        "                                                   random_state=0))\n",
        "    stack_model_names.append('Stack Gradient Boosting Machine')\n",
        "    stack_model_params.append({'n_estimators':[500, 1000, 2000], 'max_depth':[3, 5, 7]})\n",
        "    \n",
        "#    stack_models.append(SVC(probability=True, gamma='auto', tol=0.001, cache_size=200, random_state=0,\n",
        "#                             decision_function_shape='ovr', class_weight='balanced'))\n",
        "#    stack_model_names.append('Stack Support Vector Machine')\n",
        "#    stack_model_params.append({'kernel':['linear', 'rbf'], 'C':[1.0, 10.0, 100.0, 1000.0]})\n",
        "    \n",
        "#    stack_models.append(KNeighborsClassifier())\n",
        "#    stack_model_names.append('Stack K Nearest Neighbour')\n",
        "#    stack_model_params.append({'n_neighbors':[5, 10, 15], 'weights':['uniform', 'distance']})          \n",
        "\n",
        "    #########################\n",
        "    # EVALUATE STACK MODELS #\n",
        "    #########################\n",
        "    stack_fitted_models = []\n",
        "    stack_model_scores = []\n",
        "    for i in range(len(stack_models)):\n",
        "        print('Evaluating model {} of {}: {}'.format((i+1), len(stack_models), stack_model_names[i]))\n",
        "        model = stack_models[i]\n",
        "        fitted_cv, _, _ = train_model(model=model, X_train=X_train_stack, y_train=y_train,\n",
        "                                      parameters=stack_model_params[i])\n",
        "        fitted_whole_set, _, score = evaluate_model(model=fitted_cv, X_train=X_train_stack, y_train=y_train,\n",
        "                                                    X_test=X_test_stack, y_test=y_test)\n",
        "        stack_fitted_models.append(fitted_whole_set)\n",
        "        stack_model_scores.append(score)\n",
        "        print(stack_model_names[i], score)\n",
        "        \n",
        "    return stack_fitted_models, stack_model_names, stack_model_scores\n",
        "        "
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ynnhf9CcqkG"
      },
      "source": [
        "col_smiles = 'smiles'\n",
        "col_target = 'HIV_active'\n",
        "\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "METRIC_F1_SCORE = 'f1-score'\n",
        "METRIC_COHEN_KAPPA = 'Cohen kappa'\n",
        "METRIC_CONFUSION_MATRIX = 'Confusion Matrix'\n",
        "\n",
        "\n",
        "CLASSES = ['benign', 'malignant']\n",
        "TEST_RATIO = 0.2\n",
        "SEED = 0\n",
        "\n",
        "data_path = '/content/data/HIV.csv'"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndF9waW5aySQ"
      },
      "source": [
        "WORK_DIRECTORY = '/content/data'\n",
        "DATA_FILE = 'HIV.csv'\n",
        "\n",
        "data_path = os.path.join(WORK_DIRECTORY, DATA_FILE)\n",
        "n_splits = 3\n",
        "save_path = WORK_DIRECTORY\n",
        "\n",
        "\n",
        "# Read data\n",
        "X, y = read_data(data_path, col_smiles='smiles', col_target='HIV_active')\n"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpEFMRrUcXS3",
        "outputId": "11bff67d-88db-48a9-98fa-f4b0eda5936a"
      },
      "source": [
        "print(type(y))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFNt5C44euNo"
      },
      "source": [
        "X_train = X.values\n",
        "y_train_asArg = y\n"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMvY3sXPbj2c"
      },
      "source": [
        "# Get train and test set\n",
        "smiles_train, smiles_test, y_train, y_test = train_test_split(X_train, y_train_asArg, test_size = TEST_RATIO, shuffle=True, stratify=y, random_state=SEED)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DddQJSBfcjm",
        "outputId": "50db0463-1c7f-4ef9-c694-0e78f9880510"
      },
      "source": [
        "print(y_test)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I65AsH-povQ6",
        "outputId": "bec1883c-e614-49e6-8743-fcce196046dc"
      },
      "source": [
        "print(type(smiles_train))\n",
        "print((X_train))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "['CCC1=[O+][Cu-3]2([O+]=C(CC)C1)[O+]=C(CC)CC(CC)=[O+]2'\n",
            " 'C(=Cc1ccccc1)C1=[O+][Cu-3]2([O+]=C(C=Cc3ccccc3)CC(c3ccccc3)=[O+]2)[O+]=C(c2ccccc2)C1'\n",
            " 'CC(=O)N1c2ccccc2Sc2c1ccc1ccccc21' ...\n",
            " 'Cc1ccc(N2C(=O)C3c4[nH]c5ccccc5c4C4CCC(C(C)(C)C)CC4C3C2=O)cc1'\n",
            " 'Cc1cccc(N2C(=O)C3c4[nH]c5ccccc5c4C4CCC(C(C)(C)C)CC4C3C2=O)c1'\n",
            " 'CCCCCC=C(c1cc(Cl)c(OC)c(-c2nc(C)no2)c1)c1cc(Cl)c(OC)c(-c2nc(C)no2)c1']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3zr-BB0pYeR"
      },
      "source": [
        "df = pd.DataFrame(smiles_train, columns = ['Column_A'])"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcexJ2xHpjLW",
        "outputId": "adcce9bf-3c43-4624-fc4a-82eddcf2fe28"
      },
      "source": [
        "print(type(df['Column_A']))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjOX_d3QhWFn"
      },
      "source": [
        "# Extract features\n",
        "feature_train = extract_features(df['Column_A'], method=['morgan', 'mqn'],\n",
        "                                                    from_smiles=True)\n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ_zzw43rYhH"
      },
      "source": [
        "df = pd.DataFrame(smiles_test, columns = ['Column_B'])"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VDmQdUUrOZc"
      },
      "source": [
        "feature_test = extract_features(df['Column_B'], method=['morgan', 'mqn'],\n",
        "                                                    from_smiles=True)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnTID8UgruAP"
      },
      "source": [
        "# Filter out irrelevant features\n",
        "feature_train_filtered, pval_significant = filter_feature_Chi2(feature_train, y_train)\n",
        "feature_test_filtered = feature_test[:, pval_significant]\n"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJryijehr0Cw"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_train = scaler.fit_transform(feature_train_filtered)\n",
        "X_test = scaler.transform(feature_test_filtered)\n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POhCAotrr5Cf"
      },
      "source": [
        "# Build base models\n",
        "base_models, base_model_names, base_model_scores = build_base_models(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8M4yFhrvS8h"
      },
      "source": [
        "if save_path is not None:\n",
        "      # Save base models    \n",
        "      os.makedirs(os.path.join(save_path, 'base_models'), exist_ok=True)\n",
        "      for i in range(len(base_models)):\n",
        "          with open(os.path.join(save_path, 'base_models', 'base_model_' + str(i+1) + '.pkl'), 'wb') as f:\n",
        "              pickle.dump(base_models[i], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mUSaRtLviVI"
      },
      "source": [
        "# Build level 1 stack models\n",
        "stack_fitted_models, stack_model_names, stack_model_scores = build_stack_models(base_models.copy(),\n",
        "                                                                                X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "F9V0u0xjvg6e",
        "outputId": "015df2f3-8292-4492-b832-b64020a77ef9"
      },
      "source": [
        "if save_path is not None:\n",
        "    # Save base models    \n",
        "    os.makedirs(os.path.join(save_path, 'stacking_models'), exist_ok=True)\n",
        "    for i in range(len(stack_fitted_models)):\n",
        "        with open(os.path.join(save_path, 'stacking_models', stack_model_names[i] + '.pkl'), 'wb') as f:\n",
        "            pickle.dump(stack_fitted_models[i], f)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-116-894519dda000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Save base models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stacking_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_fitted_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stacking_models'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_model_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_fitted_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stack_fitted_models' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdcwrpG7vzS2"
      },
      "source": [
        "# Summarize model performance\n",
        "model_df = pd.DataFrame({'model': base_model_names + stack_model_names,\n",
        "                          METRIC_ACCURACY: [score[METRIC_ACCURACY] for score in base_model_scores]\\\n",
        "                              + [score[METRIC_ACCURACY] for score in stack_model_scores],\n",
        "                        METRIC_F1_SCORE: [score[METRIC_F1_SCORE] for score in base_model_scores]\\\n",
        "                              + [score[METRIC_F1_SCORE] for score in stack_model_scores],\n",
        "                        METRIC_COHEN_KAPPA: [score[METRIC_COHEN_KAPPA] for score in\\\n",
        "                                                    base_model_scores]\\\n",
        "                              + [score[METRIC_COHEN_KAPPA] for score in stack_model_scores],\n",
        "                        METRIC_CONFUSION_MATRIX: [score[METRIC_CONFUSION_MATRIX] for score in\\\n",
        "                                                          base_model_scores]\\\n",
        "                              + [score[METRIC_CONFUSION_MATRIX] for score in stack_model_scores]                            \n",
        "                          })\n",
        "model_df = model_df[['model', METRIC_ACCURACY, METRIC_F1_SCORE, METRIC_COHEN_KAPPA,\n",
        "                      METRIC_CONFUSION_MATRIX]]\n",
        "model_df.to_csv(os.path.join(WORK_DIRECTORY, 'summary_stacking_model.csv'), index=False)\n",
        "model_df.sort_values(by=[METRIC_ACCURACY, METRIC_F1_SCORE, METRIC_COHEN_KAPPA],\n",
        "                      ascending=False, inplace=True)\n",
        "print('Best model:\\n' + str(model_df.iloc[0]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}